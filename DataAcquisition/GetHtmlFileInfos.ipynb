{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File full of testing functions or old functions or scripts used to find out about the html structures of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32015A0228%2801%29',\n",
       "  'https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32020D2255',\n",
       "  'https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32012R0547',\n",
       "  'https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:42009D0913',\n",
       "  'https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32017R0460'],\n",
       " 537)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "serachResults_df = pd.read_csv('Search results 20240102.csv')\n",
    "\n",
    "celex_numbers = serachResults_df[\"CELEX number\"].tolist()\n",
    "celex_numbers = [s.replace(\"(\", \"%28\").replace(\")\", \"%29\") for s in celex_numbers]\n",
    "download_urls = set()\n",
    "\n",
    "for number in celex_numbers:\n",
    "    download_urls.add('https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:' + number)\n",
    "\n",
    "# remove urls there the File does not exist\n",
    "download_urls = list(download_urls.difference(set([\n",
    "    \"https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:22006A1216%2804%29\",\n",
    "    \"https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:22003A0624%2801%29\",\n",
    "    \"https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:22002A1127%2802%29\",\n",
    "    \"https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:31971G0055\",\n",
    "    \"https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:41967A0228%2801%29\",\n",
    "    \"https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:41971X0056\",\n",
    "    \"https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:31972Y1011%2801%29\",\n",
    "    \"https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:21959A1006%2801%29\",\n",
    "    \"https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:41964A0430%2801%29\",\n",
    "    \"https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:21959A1006%2802%29\",\n",
    "    \"https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:21961A0126%2801%29\",\n",
    "])))\n",
    "\n",
    "download_urls[:5], len(download_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "MIN_PARAGRAPH_LEN = 20\n",
    "\n",
    "def getHtmlText(url):\n",
    "    # Fetch HTML content from the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Failed to fetch HTML content. Status code: {response.status_code}\")\n",
    "\n",
    "def getParagraphs(text):\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    # Extract paragraphs using the find_all method\n",
    "    paragraphs = soup.find_all('p')\n",
    "\n",
    "    return [(p.get_text().replace(\"\\xa0\", \" \"), p.sourceline, p.sourcepos) for p in paragraphs]\n",
    "\n",
    "def getParagraphsFilteredByTables(text, tables):\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    # Extract paragraphs using the find_all method\n",
    "    paragraphs = soup.find_all('p')\n",
    "    res = []\n",
    "\n",
    "    nextTable = 0\n",
    "\n",
    "    for p in paragraphs:\n",
    "        p_text = p.get_text().replace(\"\\xa0\", \" \").replace(\"\\n\", \" \").strip()\n",
    "        if len(p_text) >= MIN_PARAGRAPH_LEN:\n",
    "            if nextTable == len(tables):\n",
    "                res.append((p_text, p.sourceline, p.sourcepos))\n",
    "            elif p.sourceline < tables[nextTable][0].sourceline:\n",
    "                res.append((p_text, p.sourceline, p.sourcepos))\n",
    "            elif p.find_parents('table', class_=lambda c: classFunc(c)):\n",
    "                continue\n",
    "            else:\n",
    "                res.append((p_text, p.sourceline, p.sourcepos))\n",
    "                nextTable += 1\n",
    "    return res\n",
    "\n",
    "def classFunc(c):\n",
    "    # tables without a class are just numerations in normal text, so we have to exclude them here\n",
    "    if c != None:\n",
    "        return len(c) > 0\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def getTables(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    # Extract real tables using the find_all method\n",
    "    tables = soup.find_all('table', class_=lambda c: classFunc(c))\n",
    "    return [(table, table.sourceline, table.sourcepos) for table in tables]\n",
    "\n",
    "def processHtml(url):\n",
    "    htmlText = getHtmlText(url)\n",
    "    soup = BeautifulSoup(htmlText, 'html.parser')\n",
    "    tables = getTables(htmlText)\n",
    "    filteredPs = getParagraphsFilteredByTables(htmlText, tables)\n",
    "\n",
    "    tableSentences = []\n",
    "    for j, table in enumerate(tables):\n",
    "        tableClass = table[0].get('class')[0]\n",
    "        rows = table[0].find_all('tr')\n",
    "        rowInputs = []\n",
    "        hasHeaderLine = False\n",
    "        for i, row in enumerate(rows):\n",
    "            if i == 0:\n",
    "                if tableClass == 'oj-table':\n",
    "                    row_classes = [element.get('class') for element in row.descendants if isinstance(element, type(soup.new_tag('')))]\n",
    "                    if 'oj-tbl-hdr' in row_classes:\n",
    "                        hasHeaderLine = True\n",
    "                if tableClass == 'table':\n",
    "                    row_classes = [element.get('class') for element in row.descendants if isinstance(element, type(soup.new_tag('')))]\n",
    "                    if 'tbl-hdr' in row_classes:\n",
    "                        hasHeaderLine = True\n",
    "            cells = row.find_all('td')\n",
    "            rowInputs.append(cells)\n",
    "        if hasHeaderLine:\n",
    "            for row in range(1, len(rowInputs)):\n",
    "                sentence = \"The \" + str(rowInputs[0])\n",
    "        else:\n",
    "            print(\"error\")\n",
    "\n",
    "    return filteredPs, tables\n",
    "\n",
    "\n",
    "def saveToFile(url,title):\n",
    "    # Fetch HTML content from the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        f = open(title + \".html\", \"w\")\n",
    "        f.write(response.text)\n",
    "    else:\n",
    "        print(f\"Failed to fetch HTML content. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveToFile(\"https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32011R0626\", \"file8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "documents = []\n",
    "\n",
    "for url in tqdm(download_urls[:5]):\n",
    "    documents.append(getParagraphs(getHtmlText(url)))\n",
    "\n",
    "\n",
    "with open('paragraphs_file.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# findAllTableTypes\n",
    "tableTypes = set()\n",
    "for url in tqdm(download_urls):\n",
    "    htmlText = getHtmlText(url)\n",
    "    soup = BeautifulSoup(htmlText, 'html.parser')\n",
    "    tables = getTables(htmlText)\n",
    "    for table in tables:\n",
    "        tableTypes.add(str(table[0].get('class')))\n",
    "tableTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables = getTables(getHtmlText(\"https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32009R0663\"))\n",
    "\n",
    "# print(len(tables))\n",
    "# print(\"\")\n",
    "# print(tables[0])\n",
    "# print(\"\")\n",
    "# print(tables[1])\n",
    "\n",
    "saveToFile(\"https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32023L1791\",\"test\")\n",
    "\n",
    "for url in download_urls[:10]:\n",
    "    print(url + \"    \" + str(len(processHtml(url)[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasTheId = True\n",
    "for url in tqdm(download_urls):\n",
    "    htmlText = getHtmlText(url)\n",
    "    if htmlText == None:\n",
    "        print(\"error on URL: \" + url)\n",
    "    soup = BeautifulSoup(htmlText, 'html.parser')\n",
    "\n",
    "    withId = soup.findAll('div', id='docHtml')\n",
    "    if withId == None:\n",
    "        hasTheId = False\n",
    "    elif len(withId) > 1:\n",
    "        hasTheId = False\n",
    "hasTheId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrSet = set()\n",
    "for url in download_urls:\n",
    "    htmlText = getHtmlText(url)\n",
    "    soup = BeautifulSoup(htmlText, 'html.parser')\n",
    "    sepCount = 0\n",
    "    docSepCount = 0\n",
    "    noteCount = 0\n",
    "    endCount = 0\n",
    "    hrs = soup.findAll('hr')\n",
    "    for hr in hrs:\n",
    "        # if hr in ['separator', 'oj-separator']:\n",
    "        #     sepCount += 1\n",
    "        # elif hr in ['doc-sep', 'oj-doc-sep']:\n",
    "        #     docSepCount += 1\n",
    "        # elif hr in ['note', 'oj-note']:\n",
    "        #     noteCount += 1\n",
    "        # elif hr in ['doc-end', 'oj-doc-end']:\n",
    "        #     endCount += 1\n",
    "        if hr.get('class') != None:\n",
    "            hrSet.update(hr.get('class'))\n",
    "        else:\n",
    "            print(url)\n",
    "            hrSet.add('None')\n",
    "hrSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# read df from file\n",
    "\n",
    "# make sure everything is in the right order\n",
    "df = pd.read_csv('lines.csv')\n",
    "for celexNumber, group  in df.groupby('CELEX number'):\n",
    "    i = 0\n",
    "    for index, row in group.iterrows():\n",
    "        assert(row['lineID'] == i)\n",
    "        i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('immd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3788634bf4494cb8c78f338b49d3415b443921db7d01b0c627acb9285c954547"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
